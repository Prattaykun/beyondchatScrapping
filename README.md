
---

# ğŸš€ BeyondChats â€“ End-to-End AI Blog Processing System

A full-stack, production-ready system that **scrapes blogs, enhances them using AI, and serves them through a modern frontend interface**.

This project demonstrates a **real-world scalable pipeline** involving data ingestion, AI processing, and frontend delivery â€” structured as modular services.

---

## ğŸ“Œ Project Overview

The system is divided into **three major phases**:

### **Phase 1 â€” Web Scraping**

Scrapes blog content from *BeyondChats*, extracts structured data, and stores it in MongoDB.

### **Phase 2 â€” AI Processing**

Uses Google Gemini + Google Search to rewrite, enhance, and enrich articles while preserving factual integrity.

### **Phase 3 â€” Frontend**

Displays original and AI-enhanced articles through a modern React + Vite interface.

---

## ğŸ§  Overall Architecture

```
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  BeyondChats Website â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                     (Web Scraping)
                            â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚        Phase 1: Scraper           â”‚
          â”‚  - Fetch articles                 â”‚
          â”‚  - Parse HTML (title, h1-h6, p)   â”‚
          â”‚  - Store in MongoDB               â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚        Phase 2: AI Processor      â”‚
          â”‚  - Google Search (SERP)           â”‚
          â”‚  - Gemini AI rewriting            â”‚
          â”‚  - Content enhancement            â”‚
          â”‚  - Store formatted articles       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚        Phase 3: Frontend          â”‚
          â”‚  - React + Vite                   â”‚
          â”‚  - API consumption                â”‚
          â”‚  - Clean UI rendering             â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Repository Structure

```
root/
â”‚
â”œâ”€â”€ nodejs_scrapper/          # Phase 1 â€“ Blog Scraper
â”‚   â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ server.js
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ nodejs_analyse/           # Phase 2 â€“ AI Processing
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ index.js
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ beyondchats-blogs-frontend/  # Phase 3 â€“ Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§© Phase 1 â€” Scraper (Node.js)

### ğŸ”¹ Purpose

Extracts blog articles from BeyondChats and stores structured data into MongoDB.

### ğŸ”¹ Technologies

* Node.js
* Express
* Axios
* Cheerio
* MongoDB (Mongoose)

### ğŸ”¹ Data Stored

* Title
* Slug
* Headings (`h1â€“h6`)
* Paragraphs
* Images
* Source URL

### ğŸ”¹ Run Instructions

```bash
cd nodejs_scrapper
npm install
npm start
```

This will:

* Crawl articles
* Parse content
* Save structured data in MongoDB

---

## ğŸ§  Phase 2 â€” AI Enhancement Engine

### ğŸ”¹ Purpose

Transforms raw scraped content into **human-like, SEO-optimized articles**.

### ğŸ”¹ Key Features

* Google SERP analysis
* Gemini AI rewriting
* Content restructuring
* Reference injection
* Separate collection for AI-enhanced data

### ğŸ”¹ Environment Variables

```env
MONGODB_URI=mongodb://localhost:27017/beyondchats
GEMINI_API_KEY=your_gemini_key
SERP_API_KEY=your_serpapi_key
PORT=3000
```

### ğŸ”¹ Run Commands

```bash
cd nodejs_analyse
npm install
npm start        # Start API
npm run run:phase2   # Run AI enhancement workflow
```

### ğŸ”¹ Output Collections

| Collection Name      | Description          |
| -------------------- | -------------------- |
| `articles`           | Raw scraped articles |
| `formatted_articles` | AI-enhanced versions |

---

## ğŸ¨ Phase 3 â€” Frontend (React + Vite)

### ğŸ”¹ Features

* Modern UI with Tailwind CSS
* Blog listing & detail pages
* Fetches enhanced articles from backend
* Responsive & SEO-friendly

### ğŸ”¹ Setup

```bash
cd beyondchats-blogs-frontend
npm install
npm run dev
```

App runs at:

```
http://localhost:5173
```

### ğŸ”¹ Environment Variable

```env
VITE_API_BASE_URL=http://localhost:3000
```

---

## ğŸ” End-to-End Workflow

1. **Run Scraper** â†’ Collects raw blogs
2. **Run AI Processor** â†’ Enhances content
3. **Start Frontend** â†’ Displays enriched articles
4. **User reads SEO-optimized content**

---

## âœ… Key Highlights

* Clean separation of concerns
* Production-style backend architecture
* AI-driven content enhancement
* Scalable and modular design
* Real-world engineering workflow

---

## ğŸ‘¨â€ğŸ’» Author

**Prattay Roy Chowdhury**
Full Stack Developer | AI Engineer
Built as part of an advanced full-stack & AI engineering project.

---